{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import string\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\彭伟林\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
       "1      0  Iranian general says Israel's Iron Dome can't ...\n",
       "2      2  with J Davlar 11th. Main rivals are team Polan...\n",
       "3      0  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
       "4      0  They may have a SuperBowl in Dallas, but Dalla..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['text']\n",
    "y_train = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    gas hous hit 3 39 ! ! ! ! i am go chapel hill sat\n",
       "1    iranian general say israel iron dome ca not de...\n",
       "2    davlar 11th main rival team poland hope make s...\n",
       "3    talk act amp; amp; sat decid want colleg appli...\n",
       "4    may superbowl dalla dalla ai not win superbowl...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "\n",
    "stopwords_eng = set(stopwords.words(\"english\"))\n",
    "'''\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = re.sub(\"[^a-z]\", \" \", text)\n",
    "    # words = [word for word in text.split() if word not in stopwords_eng]\n",
    "    words = [word for word in text.split()]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "'''\n",
    "def clean_text(text):\n",
    "    #1. Remove　ＨTML\n",
    "    # text = BeautifulSoup(raw_review).get_text()\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    # Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "x_train = x_train.map(clean_text)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [2312, 132, 263, 87, 2860, 5, 18, 9, 2313, 706...\n",
       "1    [1658, 59, 1086, 1778, 3629, 71, 48, 461, 264,...\n",
       "2    [592, 899, 3198, 85, 1933, 49, 29, 1212, 97, 1...\n",
       "3    [122, 480, 16, 16, 60, 616, 35, 435, 1659, 435...\n",
       "4        [7, 1213, 506, 506, 1152, 48, 58, 1213, 1299]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "num_words = 5000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = pd.Series(tokenizer.texts_to_sequences(x_train))\n",
    "x_train_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10026.000000\n",
       "mean        11.095651\n",
       "std          3.864452\n",
       "min          0.000000\n",
       "25%          8.000000\n",
       "50%         11.000000\n",
       "75%         14.000000\n",
       "max         61.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_len = x_train_seq.map(lambda ls: len(ls))\n",
    "x_train_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               328704    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 649,475\n",
      "Trainable params: 649,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout, Activation\n",
    "\n",
    "model = 0\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=num_words, output_dim=64))\n",
    "model.add(LSTM(256))\n",
    "#model.add(GlobalMaxPool1D())\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9023 samples, validate on 1003 samples\n",
      "Epoch 1/10\n",
      "9023/9023 [==============================] - 248s 27ms/step - loss: 0.5377 - acc: 0.7284 - val_loss: 0.5363 - val_acc: 0.7454\n",
      "Epoch 2/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.3854 - acc: 0.8280 - val_loss: 0.5211 - val_acc: 0.7591\n",
      "Epoch 3/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.3356 - acc: 0.8630 - val_loss: 0.5460 - val_acc: 0.7594\n",
      "Epoch 4/10\n",
      "9023/9023 [==============================] - 243s 27ms/step - loss: 0.2528 - acc: 0.9005 - val_loss: 0.6218 - val_acc: 0.7501\n",
      "Epoch 5/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.2040 - acc: 0.9223 - val_loss: 0.7311 - val_acc: 0.7461\n",
      "Epoch 6/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.1602 - acc: 0.9420 - val_loss: 0.8284 - val_acc: 0.7338\n",
      "Epoch 7/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.1221 - acc: 0.9566 - val_loss: 0.9834 - val_acc: 0.7145\n",
      "Epoch 8/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.5280 - acc: 0.7978 - val_loss: 0.6093 - val_acc: 0.7295\n",
      "Epoch 9/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.2927 - acc: 0.8825 - val_loss: 0.6996 - val_acc: 0.7318\n",
      "Epoch 10/10\n",
      "9023/9023 [==============================] - 244s 27ms/step - loss: 0.2002 - acc: 0.9233 - val_loss: 0.7895 - val_acc: 0.7355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2741dd7e5c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "model.fit(x=x_train_pad, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>id</th>\n",
       "      <th>3</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>unknwn</td>\n",
       "      <td>dec 21st 2012 will be know not as the end of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>unknwn</td>\n",
       "      <td>Yar he quite clever but aft many guesses lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>unknwn</td>\n",
       "      <td>Yeah we have Thin Lizzy here I HATE the inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>unknwn</td>\n",
       "      <td>MT @LccSy #Syria, Deir Ezzor: Ali Bashar al-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>unknwn</td>\n",
       "      <td>@MacMiller hate my life, because i can't see y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  id       3                                               text\n",
       "0 NaN   1  unknwn  dec 21st 2012 will be know not as the end of t...\n",
       "1 NaN   2  unknwn  Yar he quite clever but aft many guesses lor. ...\n",
       "2 NaN   3  unknwn  Yeah we have Thin Lizzy here I HATE the inform...\n",
       "3 NaN   4  unknwn  MT @LccSy #Syria, Deir Ezzor: Ali Bashar al-th...\n",
       "4 NaN   5  unknwn  @MacMiller hate my life, because i can't see y..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./sentence-sentiment-analysis/test.tsv\", delimiter=\"\\t\", quoting=3, header=0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    dec 21st 2012 know end world babi boom ! 2012shit\n",
       "1    yar quit clever aft mani guess lor got ask bri...\n",
       "2                      yeah thin lizzi hate informerci\n",
       "3    lccsi syria deir ezzor : ali bashar al - theeb...\n",
       "4    macmil hate life ca not see roskild festiv sat...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test.map(clean_text)\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for i in v:\n",
    "    if i == 0:\n",
    "        p.append('negative')\n",
    "    elif i == 1:\n",
    "        p.append('neutral')\n",
    "    else:\n",
    "        p.append('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'polarity':p})\n",
    "df.index += 1\n",
    "df.to_csv('sub.csv', index = True, index_label = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
